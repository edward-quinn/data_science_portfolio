---
title: "Using Machine Learning to Predict Diabetes"
author: "Edward B. Quinn, PhD, MPH"
date: "Last Updated `r Sys.Date()`"
draft: false
categories: [R,Machine Learning,Health]
format: 
  html:
    toc: true
    toc-location: left
    toc-title: Contents
bibliography: references.bib
---

# Introduction

One of the most exciting applications of machine learning is in the prediction of disease states. Here, I build a random forest classifier using a modeling framework in R called `{tidymodels}`. The random forest model will be tuned to predict diabetes diagnosis within five years.

# The Data

The data come from the National Institute of Diabetes Digestive and Kidney Diseases [@smith1988]. There are 768 Pima Indian women included in the data set. Here's a list of the variables:

-   `Pregnancies` - Number of pregnancies

-   `Glucose` - Plasma glucose in mg/dl two hours after an oral glucose tolerance test

-   `BloodPressure` - Diastolic blood pressure in mm Hg

-   `SkinThickness` - Triceps skinfold thickness; a measure of body fat

-   `Insulin` - Two hour serum insulin

-   `BMI` - Body mass index

-   `DiabetesPedigreeFunction` - A measure of family history of diabetes

-   `Age` - Measured in years

-   `Outcome` - An indicator variable for diabetes diagnosis

# Exploratory Data Analysis

Let's load our libraries.

```{r}

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(here))

# Deal with conflicting function names across packages:
tidymodels_prefer()

```

Read in the data and take a look at the numbers in @tbl-datatable. A .csv file containing the data can be downloaded [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data).

```{r}
#| message: false
#| label: tbl-datatable
#| tbl-cap: Raw Data.

df <- read_csv(here("machine_learning/diabetes.csv"))

datatable(df)

```

<br>

It is clear clicking on the filtering icons next to each variable name that there are some impossible values. For example, there are people with a BMI of zero. It is likely that missing values are encoded with a zero, so let's set values of zero for the variables `Glucose`, `BloodPressure`, `Insulin`, `BMI`, and `SkinThickness` equal to `NA` to indicate missingness.

```{r}

df <- df |> 
  mutate(across(c(Glucose,
                BloodPressure,
                Insulin,
                BMI,
                SkinThickness), ~na_if(.,0)))

```

How many missing values do we have in our dataset (@tbl-missing)?

```{r}
#| label: tbl-missing
#| tbl-cap: Number of Missing Values.

df |> 
  summarise(across(everything(), ~ sum(is.na(.x)))) |> 
  pivot_longer(everything(), values_to = "number of missing values") |> 
  mutate(`percent missing` = `number of missing values`/nrow(df)) |> 
  mutate(across(c(`percent missing`), ~round(.x*100, digits = 1))) |> 
  datatable()

```

Some columns have very few missing values, such as `Glucose`, while others, such as `Insulin`, have a very high proportion of missing values. Predictors typically cannot have missing values in machine learning applications, so we will need to estimate these missing values in our analysis pipeline.

Let's take a look at some summary statistics for our data in @tbl-summary.

```{r}
#| label: tbl-summary
#| tbl-cap: Descriptive Statistics.

# The sample n column is broken.

# We also need a column counting missingness.

df |> 
  summarise(across(colnames(df), .fns = 
                     list(n = ~n(),
                          Mean = ~mean(.x, na.rm = TRUE),
                          Median = ~median(.x, na.rm = TRUE),
                          SD = ~sd(.x, na.rm = TRUE),
                          Min = ~min(.x, na.rm = TRUE),
                          Max = ~max(.x, na.rm = TRUE),
                          q25 = ~quantile(.x, 0.25, na.rm = TRUE), 
                          q75 = ~quantile(.x, 0.75, na.rm = TRUE)
                     ))) |>  
  pivot_longer(everything(), names_sep = "_", names_to = c("variable", ".value")) |> 
  mutate(across(where(is.numeric), .fns = ~round(.x, digits = 1))) |> 
  datatable()



```

Also, note that the outcome is encoded as 0 (no diabetes) and 1 (diabetes). Let's create an additional factor variable for the outcome. This will facilitate data visualization.

```{r}

df <- df |> 
  mutate(Diagnosis = factor(Outcome, levels = c(0,1),
                          labels = c("No Diabetes","Diabetes")))

```

It is always a good idea to visualize the data as part of exploratory data analysis (@fig-pairs_plot).

```{r}
#| message: false
#| warning: false
#| fig-width: 13
#| fig-asp: 0.618
#| out-width: "100%"
#| label: fig-pairs_plot
#| fig-cap: Pairs plot for all variables in data set.


df |>
  # Don't need the "Outcome" column right now, use "Diagnosis" instead
  select(-Outcome) |>
  ggpairs(mapping = aes(color = Diagnosis, fill = Diagnosis)) +
  scale_color_brewer(type = "qual",
                     palette = "Accent") +
  scale_fill_brewer(type = "qual",
                     palette = "Accent") +
  theme_bw()

```


Note the differences in our predictors by Diagnosis, most clearly illustrated in the box plots to the right and the histograms in the bottom two rows. This indicates that our predictors may be useful in predicting onset of diabetes.

# Machine Learning

The following code will split the data into training and test sets, fit a single random forest model to the training data, and then evaluate the model on the test set using accuracy as a performance metric.

```{r}

set.seed(456)

df_split <- initial_split(data = df, prop = 0.75, strata = Diagnosis)

df_train <- training(df_split)

df_test <- testing(df_split)

# Create a model
rf_model <- 
  rand_forest(mtry = 5, min_n = 40, trees = 1000) |> 
  set_engine("ranger") |> 
  set_mode("classification")

# Create a recipe
rf_recipe <-
  recipe(Diagnosis ~ Pregnancies + Glucose + BloodPressure + SkinThickness +
                Insulin + BMI + DiabetesPedigreeFunction + Age, 
         data = df_train) |> 
  step_impute_knn(Insulin,Glucose,BloodPressure,SkinThickness,BMI,
                  DiabetesPedigreeFunction,Age) |> 
  step_normalize(all_numeric_predictors())

# Create a workflow
set.seed(456)
rf_workflow <-
  workflow() |> 
  add_model(rf_model) |> 
  add_recipe(rf_recipe)


rf_fit <- 
  fit(rf_workflow, df_train)

# Predict outcomes for the test set using the trained model
df_test_res <- predict(rf_fit, new_data = df_test |> select(-Outcome))

# Bind predictions to observed data for the test set
df_res <- bind_cols(df_test_res, df_test |>  select(Diagnosis))

# Get the accuracy
accuracy(df_res, truth = Diagnosis, estimate = .pred_class)


```

Our accuracy is 80.2%. This an improvement over the simplest model, which would consist of guessing that no one will develop diabetes. The simple model would have 65.1% accuracy, so our first pass at a random forest model does represent an improvement in accuracy over the simple model. One thing we need to check is the stability of our measure of accuracy, which could be sensitive to the random seed we used. Let's set up 5-fold cross validation (repeated 5 times) to get a more stable estimate of the accuracy of the random forest model.

```{r}

set.seed(456)
df_folds <- vfold_cv(df_train, v = 5, repeats = 5, strata = Diagnosis)
df_folds

set.seed(456)
rf_cv_fit <- 
  fit_resamples(rf_workflow, df_folds)

collect_metrics(rf_cv_fit)

```

Cross validation suggests that a more stable estimate of the accuracy of our model would be 75.8%, which is still an improvement over guessing that no one will be diagnosed with diabetes within the next five years. 

Why the drop in accuracy? This may be due to the necessarily small sample sizes in the analysis folds used in each cross validation iteration.

What can we do to improve the accuracy of our model? There are several parameters of a random forest model that can be tuned, including `mtry`, `trees`, and `min_n`. Let's try tuning to see if we can improve upon 75.8% accuracy.


```{r}

# Create a model
rf_model_tune <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = tune()) |> 
  set_engine("ranger") |> 
  set_mode("classification")

# Create a workflow
set.seed(456)
rf_workflow_tune <-
  workflow() |> 
  add_model(rf_model_tune) |> 
  add_recipe(rf_recipe)


# All operating systems
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(finetune))
set.seed(456)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(4)
registerDoParallel(cl)

system.time(
rf_tune <-
  tune_race_anova(rf_workflow_tune,
            resamples = df_folds,
            grid = 60)
)

stopCluster(cl)


rf_tune |> 
  show_best(metric = "accuracy")


```


Now let's take the optimal combination of tuning parameters and retrain the model on the entire training data set.


```{r}

# the last model
last_rf_mod <- 
  rand_forest(mtry = 2, min_n = 36, trees = 1458) |>  
  set_engine("ranger", importance = "impurity") |> 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow_tune |>  
  update_model(last_rf_mod)

# the last fit
set.seed(456)
last_rf_fit <- 
  last_rf_workflow |> 
  last_fit(df_split)

last_rf_fit |> 
  collect_metrics()


last_rf_fit |> 
  collect_predictions() |> 
  roc_curve(Diagnosis, `.pred_No Diabetes`) |> 
  autoplot()


```

Our accuracy is now 78.6%. Which variables are most important in predicting onset of diabetes within the next five years?

```{r}

suppressPackageStartupMessages(library(vip))

last_rf_fit |> 
  extract_fit_parsnip() |> 
  vip(aesthetics = list(fill = "midnightblue", alpha = 0.6)) +
  theme_classic()

```

Not surprisingly, `Glucose` is the most important variable in predicting onset of diabetes. That's reassuring since diabetes is diagnosed using glucose levels!

# Next Steps

This is a very rough first pass at predicting onset of diabetes using a random forest machine learning model. We could try to improve accuracy in the test set with additional feature engineering, tinkering with preprocessing steps, and using expanded grids when tuning parameters. That's all for now!








