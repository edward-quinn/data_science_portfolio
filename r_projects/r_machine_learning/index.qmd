---
title: "Using Machine Learning to Predict Diabetes"
author: "Edward B. Quinn, PhD, MPH"
date: "Last Updated `r Sys.Date()`"
draft: false
categories: [R,Machine Learning,Health]
format: 
  html:
    toc: true
    toc-location: left
    toc-title: Contents
bibliography: references.bib
---

# Introduction

One of the most exciting applications of machine learning is in the prediction of disease states. Here, I demonstrate the use of a random forest classifier within a modeling framework in R called `{tidymodels}`. The random forest model will be tuned to predict diabetes diagnosis within five years.

# The Data

The data come from the National Institute of Diabetes Digestive and Kidney Diseases [@smith1988]. There are 768 Pima Indian women included in the data set. Here's a list of the variables:

-   `Pregnancies` - Number of pregnancies

-   `Glucose` - Plasma glucose in mg/dl two hours after an oral glucose tolerance test

-   `BloodPressure` - Diastolic blood pressure in mm Hg

-   `SkinThickness` - Triceps skinfold thickness; a measure of body fat

-   `Insulin` - Two hour serum insulin

-   `BMI` - Body mass index

-   `DiabetesPedigreeFunction` - A measure of family history of diabetes

-   `Age` - Measured in years

-   `Outcome` - An indicator variable for diabetes diagnosis

# Exploratory Data Analysis

Let's load our libraries.

```{r}

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tidymodels))
suppressPackageStartupMessages(library(RCurl))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(DT))
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(here))

# Deal with conflicting function names across packages:
tidymodels_prefer()

```

Read in the data and take a look at the numbers in @tbl-datatable. A .csv file containing the data can be downloaded [here](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data).

```{r}
#| message: false
#| label: tbl-datatable
#| tbl-cap: Raw Data.

df <- read_csv(here("r_projects/r_machine_learning/diabetes.csv"))

datatable(df)

```

<br>

It is clear clicking on the filtering icons next to each variable name that there are some impossible values. For example, there are people with a BMI of zero. It is likely that missing values are encoded with a zero, so let's set values of zero for the variables `Glucose`, `BloodPressure`, `Insulin`, `BMI`, and `SkinThickness` equal to `NA` to indicate missingness.

```{r}

df <- df |> 
  mutate(across(c(Glucose,
                BloodPressure,
                Insulin,
                BMI,
                SkinThickness), ~na_if(.,0)))

```

How many missing values do we have in our dataset (@tbl-missing)?

```{r}
#| label: tbl-missing
#| tbl-cap: Number of Missing Values.

df |> 
  summarise(across(everything(), ~ sum(is.na(.x)))) |> 
  pivot_longer(everything(), values_to = "number of missing values") |> 
  mutate(`percent missing` = `number of missing values`/nrow(df)) |> 
  mutate(across(c(`percent missing`), ~round(.x*100, digits = 1))) |> 
  datatable()

```

Some columns have very few missing values, such as `Glucose`, while others, such as `Insulin`, have a very high proportion of missing values. Predictors typically cannot have missing values in machine learning applications, so we will need to estimate these missing values in our analysis pipeline.

Let's take a look at some summary statistics for our data in @tbl-summary.

```{r}
#| label: tbl-summary
#| tbl-cap: Descriptive Statistics.

# The sample n column is broken.

# We also need a column counting missingness.

df |> 
  summarise(across(colnames(df), .fns = 
                     list(n = ~n(),
                          Mean = ~mean(.x, na.rm = TRUE),
                          Median = ~median(.x, na.rm = TRUE),
                          SD = ~sd(.x, na.rm = TRUE),
                          Min = ~min(.x, na.rm = TRUE),
                          Max = ~max(.x, na.rm = TRUE),
                          q25 = ~quantile(.x, 0.25, na.rm = TRUE), 
                          q75 = ~quantile(.x, 0.75, na.rm = TRUE)
                     ))) |>  
  pivot_longer(everything(), names_sep = "_", names_to = c("variable", ".value")) |> 
  mutate(across(where(is.numeric), .fns = ~round(.x, digits = 1))) |> 
  datatable()



```

Also, note that the outcome is encoded as 0 (no diabetes) and 1 (diabetes). Let's create an additional factor variable for the outcome. This will facilitate data visualization.

```{r}

df <- df |> 
  mutate(Diagnosis = factor(Outcome, levels = c(0,1),
                          labels = c("No Diabetes","Diabetes")))

```

It is always a good idea to visualize the data as part of exploratory data analysis (@fig-pairs_plot).

```{r}
#| message: false
#| warning: false
#| fig-width: 13
#| fig-asp: 0.618
#| out-width: "100%"
#| label: fig-pairs_plot
#| fig-cap: Pairs plot for all variables in data set.


df |>
  # Don't need the "Outcome" column right now, use "Diagnosis" instead
  select(-Outcome) |>
  ggpairs(mapping = aes(color = Diagnosis, fill = Diagnosis)) +
  scale_color_brewer(type = "qual",
                     palette = "Accent") +
  scale_fill_brewer(type = "qual",
                     palette = "Accent") +
  theme_bw()

```


Note the differences in our predictors by Diagnosis, most clearly illustrated in the box plots to the right and the density plots in the bottom two rows. This indicates that our predictors may be useful in predicting onset of diabetes.

# Machine Learning

The following code will split the data into training and test sets, fit a single random forest model to the training data, and then evaluate the model using accuracy.

```{r}

set.seed(456)

df_split <- initial_split(data = df, prop = 0.75, strata = Diagnosis)

df_train <- training(df_split)

df_test <- testing(df_split)

# Create a model
rf_model <- 
  rand_forest(trees = 1000) |> 
  set_engine("randomForest") |> 
  set_mode("classification")

# Create a recipe
rf_recipe <-
  recipe(Diagnosis ~ Pregnancies + Glucose + BloodPressure + SkinThickness +
                Insulin + BMI + DiabetesPedigreeFunction + Age, 
         data = df_train) |> 
  step_impute_knn(Insulin,Glucose,BloodPressure,SkinThickness,BMI,
                  DiabetesPedigreeFunction,Age) |> 
  step_normalize(all_numeric())

# Create a workflow
set.seed(456)
rf_workflow <-
  workflow() |> 
  add_model(rf_model) |> 
  add_recipe(rf_recipe)


rf_fit <- 
  fit(rf_workflow, df_train)

df_test_res <- predict(rf_fit, new_data = df_test |> select(-Outcome))
df_test_res

# Bind predictions to observed data
df_res <- bind_cols(df_test_res, df_test |>  select(Diagnosis))


accuracy(df_res, truth = Diagnosis, estimate = .pred_class)


```

Our accuracy is 78%, which is substantially better than simply guessing that everyone will not be diagnosed with diabetes. That simple model would have 65% accuracy, so our first pass at a random forest model does represent an improvement in accuracy over the most parsimonious predictive model possible.

There are several parameters of a random forest model that can be tuned, including `mtry`, `trees`, and `min_n`. See documentation for an explanation of these. Let's try tuning these parameters to see if we can improve upon 78% accuracy.

First, set up cross validation and run model using 5-fold cross validation five times.

```{r}



set.seed(456)
df_folds <- vfold_cv(df_train, v = 5, repeats = 5)
df_folds

set.seed(456)
rf_cv_fit <- 
  fit_resamples(rf_workflow, df_folds)

collect_metrics(rf_cv_fit)

```


Now let's try tuning the `mtry` and `min_n` parameters.

```{r}


# Create a model
rf_model_tune <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("randomForest") |> 
  set_mode("classification")

# Create a workflow
set.seed(456)
rf_workflow <-
  workflow() |> 
  add_model(rf_model_tune) |> 
  add_recipe(rf_recipe)



# All operating systems
library(doParallel)
set.seed(456)
# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)

system.time(
rf_tune <-
  tune_grid(rf_workflow,
            resamples = df_folds,
            grid = 11)
)

stopCluster(cl)



```













